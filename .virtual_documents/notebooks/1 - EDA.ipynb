








# (this is not an exhaustive list of libraries)
import pandas as pd
import numpy as np
import os
import json
from pprint import pprint
from functions_variables import encode_tags







def flatten_json(json_data, key_prefix="", sep="."):
    """
    Flattens a nested JSON-like dictionary, except for the 'tags' field.
    Args:
        json_data: The JSON data (dictionary) to flatten.
        key_prefix:  A prefix to add to the keys (used for recursion).
        sep: The separator to use between nested keys.
    Returns:
        A flattened dictionary.
    """
    flattened = {}
    for k, v in json_data.items():
        new_key = key_prefix + k if key_prefix else k
        if k == "tags":
            flattened[new_key] = v
        elif isinstance(v, dict):
            flattened.update(flatten_json(v, new_key + sep, sep=sep))
        elif isinstance(v, list):
            for i, item in enumerate(v):
                if isinstance(item, dict):
                    flattened.update(flatten_json(item, new_key + f"[{i}]" + sep, sep=sep))
                else:
                    flattened[new_key + f"[{i}]"] = item
        elif v is None:
            flattened[new_key] = None
        else:
            flattened[new_key] = v
    return flattened

def extract_data_from_json_files(directory):
    """
    Extracts data from JSON files in the specified directory and combines them into a single DataFrame.
    
    Args:
        directory: The directory containing the JSON files.
        
    Returns:
        A Pandas DataFrame containing the combined data from all JSON files.
    """
    all_data = []
    
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            file_path = os.path.join(directory, filename)
            try:
                with open(file_path, "r") as f:
                    data = json.load(f)
                # Extract the 'results' list from the JSON data
                results = data.get("data", {}).get("results", [])
                # Flatten each result and store in a list
                flattened_data = [flatten_json(result) for result in results]
                all_data.extend(flattened_data)
            except (FileNotFoundError, json.JSONDecodeError) as e:
                print(f"Error processing file {file_path}: {e}")
    
    # Create the Pandas DataFrame
    combined_df = pd.DataFrame(all_data)
    
    return combined_df

# Directory containing the JSON files
# Nancy Directory (comment out)
directory = "../data/"

# Fitsum Directory (comment out)
# directory = "/Users/fitsumbahlebi/Desktop/repo2/LHL-supervised-learning-midterm/data"

# Extract data and create the combined DataFrame
combined_df = extract_data_from_json_files(directory)

# Display the combined DataFrame
print(combined_df.head())











combined = combined_df.groupby('property_id').agg({
    'list_price': 'mean',  # Average price if duplicates exist
    'description.beds': lambda x: x.mode().iloc[0] if not x.mode().empty else x.max(),
    'description.baths': lambda x: x.mode().iloc[0] if not x.mode().empty else x.max(),
    'description.year_built': 'max',  # Use max for year if duplicates exist
    'description.baths_full': lambda x: x.mode().iloc[0] if not x.mode().empty else x.max(),
    'description.baths_half': lambda x: x.mode().iloc[0] if not x.mode().empty else x.max(),
    'description.lot_sqft': 'max',  # Max lot square footage
    'description.sqft': 'max',  # Max square footage
    'description.sub_type': lambda x: x.mode().iloc[0] if not x.mode().empty else x.max(),
    'description.baths_1qtr': lambda x: x.mode().iloc[0] if not x.mode().empty else x.max(),
    'description.garage': 'max',  # Max number of garages
    'description.stories': 'max',  # Max number of stories
    'description.type': lambda x: x.mode().iloc[0] if not x.mode().empty else x.max(),  # Use mode for type
    'last_update_date': 'max'  # Keep the latest update date
}).reset_index()
combined.shape





columns_to_consider = ['list_date', 'year_built', 'sold_date', 'sold_price', 'baths_full', 
                       'baths_half', 'lot_sqft', 'sqft', 'baths_total', 'garage_spaces', 
                       'num_stories', 'beds', 'property_type', 'property_id', 'is_price_reduced', 
                       'is_new_listing', 'product_brand', 'price_reduction', 'postal_code', 
                       'state', 'longitude', 'latitude', 'city', 'state_code', 'address', 
                       'county', 'has_matterport', 'price_reduction', 
                       'lot_sqft', 'sold_price', 'sqft', 'num_stories', 
                       'beds', 'garage_spaces']  # Specify the columns excluding 'property_tags'

combined_df_renamed = combined_df_renamed.drop_duplicates(subset=columns_to_consider)





print(f"Unique DataFrame shape: {combined_df_renamed.shape}")





columns_to_drop = [
    # Photo-related columns
    'primary_photo.href', 'photos[0].tags', 'photos[0].href',
    'photos[1].tags', 'photos[1].href', 'photos[2].tags', 
    'photos[2].href', 'photos[3].tags', 'photos[3].href',
    'photos[4].tags', 'photos[4].href', 'primary_photo', 'photos',

    # Office and agent related columns
    'source.agents[0].office_name', 'source.agents[1].office_name',
    'source.agents[2].office_name', 'source.agents[3].office_name',
    'community.advertisers[0].office.hours',
    'community.advertisers[0].office.phones[0].number',
    'community.advertisers[0].office.phones[0].type',
    'community.description.name',
    
    # Virtual tour related columns
    'virtual_tours[0].href', 'virtual_tours[0].type', 'virtual_tours',
    
    # Other listings related columns
    'other_listings.rdc[0].listing_id', 'other_listings.rdc[0].status', 
    'other_listings.rdc[4].listing_id', 'other_listings.rdc[4].status',
    'other_listings.rdc[5].listing_id', 'other_listings.rdc[5].status',
    'other_listings.rdc[6].listing_id', 'other_listings.rdc[6].status',
    'other_listings.rdc[7].listing_id', 'other_listings.rdc[7].status',
    'other_listings.rdc[8].listing_id', 'other_listings.rdc[8].status',
    'other_listings.rdc[9].listing_id', 'other_listings.rdc[9].status',
    'other_listings.rdc[10].listing_id', 'other_listings.rdc[10].status',
    'other_listings.rdc[11].listing_id', 'other_listings.rdc[11].status',
    'other_listings.rdc[12].listing_id', 'other_listings.rdc[12].status',
    'other_listings.rdc[13].listing_id', 'other_listings.rdc[13].status',
    'other_listings.rdc[14].listing_id', 'other_listings.rdc[14].status',
    'other_listings.rdc[15].listing_id', 'other_listings.rdc[15].status',
    'other_listings', 'other_listings.rdc[0].primary', 'other_listings.rdc[1].primary',
    'other_listings.rdc[1].listing_id', 'other_listings.rdc[1].status', 
    'other_listings.rdc[2].listing_id', 'other_listings.rdc[2].status', 
    'other_listings.rdc[3].listing_id', 'other_listings.rdc[3].status', 
    'other_listings.rdc[3].primary', 'other_listings.rdc[2].primary',
    'other_listings.rdc[2].listing_key', 'status','source.type',
    'other_listings.rdc[3].listing_key', 'branding[0].photo',
    'other_listings.rdc[4].listing_key', 'other_listings.rdc[4].primary',
    'other_listings.rdc[5].listing_key', 'other_listings.rdc[5].primary',
    'other_listings.rdc[6].listing_key', 'other_listings.rdc[6].primary',
    'other_listings.rdc[7].listing_key', 'other_listings.rdc[7].primary',
    'other_listings.rdc[8].listing_key', 'other_listings.rdc[8].primary',
    'other_listings.rdc[9].listing_key', 'other_listings.rdc[9].primary',
    'other_listings.rdc[10].listing_key', 'other_listings.rdc[10].primary',
    'other_listings.rdc[11].listing_key', 'other_listings.rdc[11].primary',
    'other_listings.rdc[12].listing_key', 'other_listings.rdc[12].primary',
    'other_listings.rdc[13].listing_key', 'other_listings.rdc[13].primary',
    'other_listings.rdc[14].listing_key', 'other_listings.rdc[14].primary',
    'other_listings.rdc[15].listing_key', 'other_listings.rdc[15].primary',
    'other_listings.rdc[0].listing_key', 'other_listings.rdc[1].listing_key',

    # Source and metadata related columns
    'source.plan_id', 'source.spec_id', 'source', 'products',
    'last_update_date', 'lead_attributes.show_contact_an_agent',
    'location.county.fips_code', 'location.address.coordinate',
    'flags.is_foreclosure','permalink',
    
    # Empty Columns
    'baths_1qtr', 'location.county', 'property_name', 'is_subdivision',
    'is_for_rent', 'is_new_construction', 'is_pending', 'is_plan',
    'is_coming_soon', 'community', 'baths_1qtr', 'is_contingent',
    'open_houses', 'description.name', 'branding[0].name', 'description.baths_1qtr',
    'branding[0].type', 'flags.is_new_construction', 'flags.is_for_rent',
    'flags.is_subdivision', 'flags.is_contingent', 'flags.is_pending', 'flags.is_plan',
    'flags.is_coming_soon', 'listing_id', 'location.street_view_url'
]
# Drop the specified columns from the DataFrame
combined_after_drop_df = combined_df.drop(columns=columns_to_drop, axis=1,errors='ignore')





# Check unique values for each column in non_zero_cols

for col in combined_after_drop_df.columns:
    print(f"\nColumn: {col}")
    try:
        # Handle list-type columns differently
        if col == 'property_tags':
            # Get all unique tags across all lists
            all_tags = set()
            for tags in combined_df[col].dropna():
                if isinstance(tags, list):
                    all_tags.update(tags)
            print("Unique tags found:", len(all_tags))
            print("Sample of tags:", list(all_tags)[:5])
        else:
            unique_values = combined_df[col].unique()
            print(f"Count of unique values: {len(unique_values)}")
            print("Sample values:", unique_values[:5])
    except Exception as e:
        print(f"Could not process column due to: {str(e)}")
        
    if col != 'property_tags':  # Skip checking tags
        try:
            unique_values = combined_df[col].unique()
            print(f"Count of unique values: {len(unique_values)}")
            print("Sample values:", unique_values[:5])
        except Exception as e:
            print(f"Could not process column due to: {str(e)}")
    print("-" * 50)


display(combined_after_drop_df.shape)
display("====================================")
# Display the updated DataFrame
display(combined_after_drop_df.head())
display("====================================")
display(combined_after_drop_df.columns)
display("====================================")
display (combined_after_drop_df.info())


# Get columns with less than 500 nulls and their null counts
null_counts = combined_after_drop_df.isnull().sum()
columns_with_less_than_500_nulls = null_counts[null_counts > 500]
display(columns_with_less_than_500_nulls.sort_values(ascending=False))









new_column_names = {
    'source.type': 'listing_source',
    'tags': 'property_tags',
    'permalink': 'full_address',
    'status': 'listing_status',
    'list_date': 'list_date',
    'description.year_built': 'year_built',
    'description.baths_3qtr': 'baths_3qtr',
    'description.sold_date': 'sold_date',
    'description.sold_price': 'sold_price',
    'description.baths_full': 'baths_full',
    'description.name' : 'property_name',
    'description.baths_half': 'baths_half',
    'description.lot_sqft': 'lot_sqft',
    'description.sqft': 'sqft',
    'description.baths': 'baths_total',
    'description.sub_type': 'property_subtype',
    'description.baths_1qtr': 'baths_1qtr',
    'description.garage': 'garage_spaces',
    'description.stories': 'num_stories',
    'description.beds': 'beds',
    'description.type': 'property_type',
    'branding[0].name': 'listing_company_name',
    'branding[0].type': 'listing_company_type',
    'list_price': 'list_price',
    'property_id': 'property_id',
    'flags.is_new_construction' : 'is_new_construction',
    'flags.is_for_rent' : 'is_for_rent', 
    'flags.is_subdivision': 'is_subdivision',
    'flags.is_contingent': 'is_contingent', 
    'flags.is_pending' : 'is_pending', 
    'flags.is_plan' : 'is_plan',
    'flags.is_coming_soon' : 'is_coming_soon',
    'flags.is_price_reduced': 'is_price_reduced',
    'flags.is_new_listing': 'is_new_listing',
    'products.brand_name': 'product_brand',
    'listing_id': 'listing_id',
    'price_reduced_amount': 'price_reduction',
    'location.address.postal_code': 'postal_code',
    'location.address.state': 'state',
    'location.address.coordinate.lon': 'longitude',
    'location.address.coordinate.lat': 'latitude',
    'location.address.city': 'city',
    'location.address.state_code': 'state_code',
    'location.address.line': 'address',
    'location.street_view_url': 'street_view_url',
    'location.county.name': 'county',
    'matterport': 'has_matterport'
}

combined_df_renamed= combined_after_drop_df.rename(columns=new_column_names)

print(combined_df_renamed.columns)  # Verify the column names have changed
print("====================================")
combined_df_renamed.head()  # Display the first few rows of the DataFrame





from pandas import Int64Dtype

# Convert to boolean
def booleanConverter(column):
    combined_df_renamed[column] = combined_df_renamed[column].map({'True':True, 'False':False})
    return column
# Convert to datetime
combined_df_renamed['list_date'] = pd.to_datetime(combined_df_renamed['list_date'], errors='coerce', utc = True)  # 'coerce' will turn invalid dates into NaT
combined_df_renamed['sold_date'] = pd.to_datetime(combined_df_renamed['sold_date'], errors='coerce', utc = True)

# Integer columns (using Int64 to handle missing values)
numerical_features = ['year_built', 'baths_3qtr', 'baths_full', 'baths_half','baths_total' , 'sold_price', 'price_reduction','garage_spaces', 'num_stories', 'beds']
for col in numerical_features:
    combined_df_renamed[col] = combined_df_renamed[col].astype(Int64Dtype())

#Boolean Converter function
bool_like = ['is_price_reduced', 'is_new_listing', 'has_matterport']
for column in bool_like:
    combined_df_renamed[column] = np.where(combined_df_renamed[column].isin(['True', 'False']), combined_df_renamed[column].map({'True': True, 'False': False}), combined_df_renamed[column])
    combined_df_renamed[column] = combined_df_renamed[column].fillna(False).astype(int) # Convert to 0 and 1
# Convert to category
category_cols = ['property_subtype', 'property_type', 'city', 'state', 'state_code', 'county']
for col in category_cols:
    combined_df_renamed[col] = combined_df_renamed[col].astype('category')
# Print dtypes to verify
print(combined_df_renamed.dtypes)
print("====================================")
display(combined_df_renamed.head())


combined_df_renamed.info()


combined_df_renamed.describe()


# Correct the typo
combined_df_renamed['property_type'] = combined_df_renamed['property_type'].replace('condos', 'condo')

# Ensure categories are identical before filling NaNs
# Convert to string before filling NaNs
if 'property_subtype' in combined_df_renamed.columns:
    combined_df_renamed['property_subtype'] = combined_df_renamed['property_subtype'].astype(str)
    new_categories = combined_df_renamed['property_subtype'].unique()
    existing_categories = combined_df_renamed['property_type'].cat.categories
    categories_to_add = [cat for cat in new_categories if cat not in existing_categories]
    combined_df_renamed['property_type'] = combined_df_renamed['property_type'].cat.add_categories(categories_to_add)
    combined_df_renamed['property_type'].fillna(combined_df_renamed['property_subtype'], inplace=True)
    combined_df_renamed.drop(columns=['property_subtype'], inplace=True)

# Drop 'list_price' inorder to predict the price of houses not yet listed, too close to the sale price
combined_df_renamed = combined_df_renamed.drop(columns=['list_price'], errors='ignore')

# Convert datetime columns before handling other numeric columns
combined_df_renamed['list_date'] = pd.to_datetime(combined_df_renamed['list_date'], errors='coerce')
combined_df_renamed['sold_date'] = pd.to_datetime(combined_df_renamed['sold_date'], errors='coerce')

# Drop rows with missing 'sold_price' values
combined_df_renamed = combined_df_renamed.dropna(subset=['sold_price'])

#Impute longitude and lattitude
combined_df_renamed['latitude'] = combined_df_renamed['latitude'].fillna(combined_df_renamed.groupby('postal_code')['latitude'].transform('median'))
combined_df_renamed['longitude'] = combined_df_renamed['longitude'].fillna(combined_df_renamed.groupby('postal_code')['longitude'].transform('median'))

# Impute year_built with median
combined_df_renamed['year_built'].fillna(combined_df_renamed['year_built'].median(), inplace=True)

# Impute baths_3qtr and baths_half with 0
combined_df_renamed['baths_3qtr'].fillna(0, inplace=True)
combined_df_renamed['baths_half'].fillna(0, inplace=True)
# Impute missing values in 'garage_spaces' with the 0
combined_df_renamed['garage_spaces'].fillna(0, inplace=True)

#Impute with most frequent values.
combined_df_renamed["product_brand"].fillna(combined_df_renamed["product_brand"].mode()[0], inplace = True)
# Impute baths_full and baths_total with median
combined_df_renamed['baths_full'].fillna(combined_df_renamed['baths_full'].median(), inplace=True)
combined_df_renamed['baths_total'].fillna(combined_df_renamed['baths_total'].median(), inplace=True)

# Impute lot_sqft and sqft with median
combined_df_renamed['lot_sqft'].fillna(combined_df_renamed['lot_sqft'].median(), inplace=True)
combined_df_renamed['sqft'].fillna(combined_df_renamed['sqft'].median(), inplace=True)

# Impute garage_spaces with 0
combined_df_renamed['garage_spaces'].fillna(0, inplace=True)

# Impute num_stories with mode
combined_df_renamed['num_stories'].fillna(combined_df_renamed['num_stories'].mode()[0], inplace=True)

# Impute beds with median
combined_df_renamed['beds'].fillna(combined_df_renamed['beds'].median(), inplace=True)

# Impute property_type with mode
combined_df_renamed['property_type'].fillna(combined_df_renamed['property_type'].mode()[0], inplace=True)

# For each postal code impute the date with the median sold_date for properties in that postal code
combined_df_renamed['list_date'] = combined_df_renamed['list_date'].fillna(
    combined_df_renamed.groupby('postal_code')['sold_date'].transform(lambda x: x.median() if not x.empty else pd.NaT)
)

# Fill any remaining NaT values with the overall median
if combined_df_renamed['list_date'].isna().any():
    combined_df_renamed['list_date'] = combined_df_renamed['list_date'].fillna(combined_df_renamed['list_date'].median())

# Impute price_reduction (conditional)
combined_df_renamed['price_reduction'] = np.where(combined_df_renamed['is_price_reduced'], combined_df_renamed['price_reduction'].fillna(combined_df_renamed['price_reduction'].median()), 0)

#Handle the updated list_date, listing_id, address and county
# Impute list_date
# For each postal code impute the date with the median sold_date for properties in that postal code
combined_df_renamed['list_date'] = combined_df_renamed['list_date'].fillna(combined_df_renamed.groupby('postal_code')['sold_date'].transform('median'))

#Impute address
# Use a lambda function to handle cases where there's no mode
combined_df_renamed['address'] = combined_df_renamed['address'].fillna(
    combined_df_renamed.groupby('county')['address'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)
)

#Impute County
# Use a lambda function to handle cases where there's no mode
combined_df_renamed['county'] = combined_df_renamed['county'].fillna(
    combined_df_renamed.groupby('postal_code')['county'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)
)

#Impute City
# Use a lambda function to handle cases where there's no mode
combined_df_renamed['city'] = combined_df_renamed['city'].fillna(
    combined_df_renamed.groupby('postal_code')['city'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)
)
# Attempt to impute remaining latitude/longitude with overall median
combined_df_renamed['latitude'].fillna(combined_df_renamed['latitude'].median(), inplace=True)
combined_df_renamed['longitude'].fillna(combined_df_renamed['longitude'].median(), inplace=True)

print(combined_df_renamed.isnull().sum().sort_values(ascending=False))  # Verify that null values have been handled



# Create a new column 'age_built' by subtracting 'year_built' from the current year
now = pd.to_datetime('now')  # get current datetime
combined_df_renamed['age_built'] = (now.year - combined_df_renamed['year_built']).astype('float64')


# Check information
combined_df_renamed.info()


# Descriptive statistics
combined_df_renamed.describe()





import pandas as pd

def check_columns_with_lists_or_dicts(df):
    """
    Checks whether columns in the DataFrame contain either lists or dictionaries (or both).
    
    Args:
        df: The Pandas DataFrame to check.
        
    Returns:
        A list of column names where either lists or dictionaries (or both) are present.
    """
    columns_with_lists_or_dicts = []
    
    for col in df.columns:
        contains_list = any(isinstance(item, list) for item in df[col])
        contains_dict = any(isinstance(item, dict) for item in df[col])
        if contains_list or contains_dict:
            columns_with_lists_or_dicts.append(col)
    
    return columns_with_lists_or_dicts

# Call the function with the DataFrame
columns_with_lists_or_dicts = check_columns_with_lists_or_dicts(combined_df_renamed)
print(columns_with_lists_or_dicts)














from collections import Counter

# Flatten the lists of tags in the 'property_tags' column, handling None values
all_tags = []
for sublist in combined_df_renamed['property_tags'].dropna():
	if isinstance(sublist, list):
		all_tags.extend(sublist)

# Count the occurrences of each tag
tag_counts = Counter(all_tags)

# Convert the counts to a DataFrame for better visualization
tag_counts_df = pd.DataFrame(tag_counts.items(), columns=['Tag', 'Count'])
tag_counts_df = tag_counts_df.sort_values('Count', ascending=False)

# Display the tag counts
display(tag_counts_df.sort_values('Count', ascending=False).tail(60))





from collections import Counter
import matplotlib.pyplot as plt


#1 Convert to pandas series
all_tags = []
for tag_list in combined_df_renamed['property_tags']:
    if isinstance(tag_list, list):
        all_tags.extend(tag_list)

# Count the frequency of each tag
tag_counts = Counter(all_tags)

tag_counts_series = pd.Series(tag_counts).sort_values(ascending=False)

# Select the value from the X amount from the data.
minCount = int(tag_counts_series.values[80]) # the value of tag with the rank 15 will be taken

print("Min Count/ Threshold", minCount) # see the value to prevent making mistakes.

# Plot code for visualization
plt.figure(figsize=(12,6))
plt.plot(tag_counts_series.values)
plt.xlabel("Tag Rank")
plt.ylabel("Tag Frequency")
plt.title("Tag Frequency Distribution (Line Plot)")
plt.show()








from sklearn.model_selection import train_test_split

def preprocess_data(df):
    """
    - Splits data into train and test sets.
    - Applies target encoding to 'city', 'state', and 'county'.
    - Applies one-hot encoding to 'property_tags' (with a frequency threshold).
    - Returns cleaned train and test sets.
    """

    # Train-Test Split
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

    # Target Encoding (City, State, County)
    target_cols = ['city', 'state', 'county']
    overall_mean = train_df['sold_price'].mean()

    for col in target_cols:
        mapping = train_df.groupby(col)['sold_price'].mean()
        train_df[col] = train_df[col].map(mapping)
        test_df[col] = test_df[col].map(mapping).fillna(overall_mean)

    # One-Hot Encoding for Property Tags
    if 'property_tags' in df.columns:
        # Flatten the list of all tags
        all_tags = train_df['property_tags'].explode()
        tag_counts = all_tags.value_counts()

        # Keep tags that appear at least 50 times
        frequent_tags = tag_counts[tag_counts >= 50].index.tolist()

        def encode_tags(tag_list):
            if not isinstance(tag_list, list):  # Handle None or unexpected types
                tag_list = []
            return pd.Series({tag: 1 if tag in tag_list else 0 for tag in frequent_tags})

        # Apply to train and test
        train_tags = train_df['property_tags'].apply(encode_tags)
        test_tags = test_df['property_tags'].apply(encode_tags)

        # Drop original property_tags column
        train_df = train_df.drop(columns=['property_tags']).join(train_tags)
        test_df = test_df.drop(columns=['property_tags']).join(test_tags)

    return train_df, test_df





# Example usage:
train_df1, test_df1 = preprocess_data(combined_df_renamed)
train_df1.shape, test_df1.shape


display(train_df1.head())
display("====================================")
display(test_df1.head())





# import, join and preprocess new data here






import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew
import numpy as np



numerical_features = ['year_built', 'age_built', 'lot_sqft', 'sqft', 'baths_total', 'garage_spaces', 'num_stories', 'beds', 'sold_price', 'price_reduction']

# Convert numerical features to float64
for col in numerical_features:
    combined_df_renamed[col] = combined_df_renamed[col].astype('float64')




# 1. Distribution of Numerical Variables
plt.figure(figsize=(15, 10))
for i, feature in enumerate(numerical_features):
    plt.subplot(4, 3, i + 1)  # Adjusted to 4x3 grid
    sns.histplot(combined_df_renamed[feature].dropna(), kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()





# 2. Scatter Plots (Numerical vs. Target)
numerical_features_for_scatter = ['year_built', 'lot_sqft', 'sqft', 'baths_total', 'garage_spaces', 'num_stories', 'beds', 'price_reduction']
plt.figure(figsize=(15, 10))
for i, feature in enumerate(numerical_features_for_scatter):
    plt.subplot(3, 3, i + 1)
    sns.scatterplot(x=combined_df_renamed[feature].dropna(), y=combined_df_renamed['sold_price'])
    plt.title(f'{feature} vs. sold_price')
plt.tight_layout()
plt.show()





# 3. Heatmap or Correlation Analysis
correlation_matrix = combined_df_renamed[numerical_features].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()





# Calculate skewness for each numerical feature individually
for col in numerical_features:
    try:
        col_skewness = skew(combined_df_renamed[col].dropna())
        print(f"Skewness for column '{col}': {col_skewness}")
    except Exception as e:
        print(f"Error calculating skewness for column '{col}': {e}")


#4. Identifying skewed features
from scipy.stats import skew

# Calculate skewness for each numerical feature in the list
skewed_feats = pd.Series({col: skew(combined_df_renamed[col].dropna()) for col in numerical_features})


skewed_feats = skewed_feats[abs(skewed_feats) > 0.5]
skewed_feats = skewed_feats.sort_values(ascending=False)
print("Skewed features: %s" % skewed_feats)











import os
from sklearn.preprocessing import StandardScaler

# Make sure the 'processed' directory exists
processed_folder = '../data/processed'
os.makedirs(processed_folder, exist_ok=True)

# Extract numerical columns to scale (drop 'sold_price' as it's the target)
numerical_cols = train_df1.select_dtypes(include=['float64', 'int64']).columns.tolist()
numerical_cols.remove('sold_price')  # We don't want to scale the target variable

# Initialize the scaler
scaler = StandardScaler()

# Scale training and testing data
train_scaled = train_df1[numerical_cols].copy()
test_scaled = test_df1[numerical_cols].copy()

# Fit and transform the training data, then transform the test data
train_scaled[numerical_cols] = scaler.fit_transform(train_scaled[numerical_cols])
test_scaled[numerical_cols] = scaler.transform(test_scaled[numerical_cols])


# Concatenate all columns at once instead of inserting individually
train_scaled = pd.concat([train_scaled, train_df1[['sold_price']]], axis=1)
test_scaled = pd.concat([test_scaled, test_df1[['sold_price']]], axis=1)

# Save the scaled data to CSV files
train_scaled.to_csv(os.path.join(processed_folder, 'train_scaled.csv'), index=False)
test_scaled.to_csv(os.path.join(processed_folder, 'test_scaled.csv'), index=False)

print("Data scaling and saving completed!")

