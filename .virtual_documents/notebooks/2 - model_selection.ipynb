


# IMPORTS
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_absolute_error, accuracy_score, mean_squared_error, r2_score





# Load processed data
processed_folder = '../data/processed'
file_path = os.path.join(processed_folder, 'train_scaled.csv')  # Use the already scaled dataset
data = pd.read_csv(file_path)

# Define target and features
target_column = 'sold_price'  # since we are looking at house prices
y = data[target_column]
X = data.drop(columns=[target_column])

# Categorical Target Encoding (Low, Medium, High)
bins = [0, 100000, 700000, float('inf')]  # Example bins
labels = ['Low', 'Medium', 'High']
y_binned = pd.cut(y, bins=bins, labels=labels)

# Convert categories to integers
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_binned)






# Train-test split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Handle missing values with imputation
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Convert the imputed data back into a DataFrame and assign column names
X_train_imputed_df = pd.DataFrame(X_train_imputed, columns=X.columns)  # Retaining original column names
X_test_imputed_df = pd.DataFrame(X_test_imputed, columns=X.columns)    # Retaining original column names

# Save the DataFrames to CSV
X_train_imputed_df.to_csv('X_train_imputed.csv', index=False)
X_test_imputed_df.to_csv('X_test_imputed.csv', index=False)

# Also save the target (y) as before
pd.DataFrame(y_train_imputed).to_csv('y_train_imputed.csv', index=False)
pd.DataFrame(y_test_imputed).to_csv('y_test_imputed.csv', index=False)


# Ensure target variable has no missing values
y_imputer = SimpleImputer(strategy='most_frequent')
y_train_imputed = y_imputer.fit_transform(y_train.reshape(-1, 1)).ravel()
y_test_imputed = y_imputer.transform(y_test.reshape(-1, 1)).ravel()







# Initialize models
models = {
    'Linear Regression': LinearRegression(),
    'Support Vector Machine': SVR(),
    'Random Forest': RandomForestRegressor(),
}

# Train models and evaluate performance
results = {}

for model_name, model in models.items():
    model.fit(X_train_imputed, y_train_imputed)
    y_pred = model.predict(X_test_imputed)

    # Mean Absolute Error (MAE)
    mae = mean_absolute_error(y_test_imputed, y_pred)

    # Root Mean Squared Error (RMSE)
    rmse = np.sqrt(mean_squared_error(y_test_imputed, y_pred))

    # R-Squared (R^2)
    r2 = r2_score(y_test_imputed, y_pred)

    # Adjusted R-Squared (Adjusted R^2)
    n = X_test_imputed.shape[0]  # Number of test samples
    p = X_test_imputed.shape[1]  # Number of features
    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    # Store the results
    results[model_name] = {
        'MAE': mae,
        'RMSE': rmse,
        'R^2': r2,
        'Adjusted R^2': adjusted_r2
    }

# Display results
results_df = pd.DataFrame(results).T
print(results_df)









# Train a RandomForestClassifier for feature importance analysis
rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train_imputed, y_train_imputed)
y_pred_class = rfc.predict(X_test_imputed)

# Evaluate classification accuracy
accuracy = accuracy_score(y_test_imputed, y_pred_class)
print(f'Model accuracy score: {accuracy:.4f}')








importances = rfc.feature_importances_
feature_names = X.columns
indices = np.argsort(importances)[::-1]

# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Display the top 20 features and their importances
top_n = 20
for f in range(top_n):
    print(f"{feature_names[indices[f]]}: {importances[indices[f]]:.4f}")

# Plot the top 20 feature importances
plt.figure(figsize=(10, 6))
plt.title("Top 20 Feature Importances")
plt.barh(range(top_n), importances[indices[:top_n]], align="center")
plt.yticks(range(top_n), feature_names[indices[:top_n]])
plt.xlabel("Importance")
plt.show()









# gather evaluation metrics and compare results
# Display results
results_df = pd.DataFrame(results).T
print(results_df)









# perform feature selection 
# refit models
# gather evaluation metrics and compare to the previous step (full feature set)



