


# develop your custom functions here

def custom_cross_validation(training_data, n_splits =5):
    '''creates n_splits sets of training and validation folds

    Args:
      training_data: the dataframe of features and target to be divided into folds
      n_splits: the number of sets of folds to be created

    Returns:
      A tuple of lists, where the first index is a list of the training folds, 
      and the second the corresponding validation fold

    Example:
        >>> output = custom_cross_validation(train_df, n_splits = 10)
        >>> output[0][0] # The first training fold
        >>> output[1][0] # The first validation fold
        >>> output[0][1] # The second training fold
        >>> output[1][1] # The second validation fold... etc.
    '''

    return training_folds, validation_folds

def hyperparameter_search(training_folds, validation_folds, param_grid):
    '''outputs the best combination of hyperparameter settings in the param grid, 
    given the training and validation folds

    Args:
      training_folds: the list of training fold dataframes
      validation_folds: the list of validation fold dataframes
      param_grid: the dictionary of possible hyperparameter values for the chosen model

    Returns:
      A list of the best hyperparameter settings based on the chosen metric

    Example:
        >>> param_grid = {
          'max_depth': [None, 10, 20, 30],
          'min_samples_split': [2, 5, 10],
          'min_samples_leaf': [1, 2, 4],
          'max_features': ['sqrt', 'log2']} # for random forest
        >>> hyperparameter_search(output[0], output[1], param_grid = param_grid) 
        # assuming 'ouput' is the output of custom_cross_validation()
        [20, 5, 2, 'log2'] # hyperparams in order
    '''

    return hyperparameters






# imports

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd



# perform tuning and cross validation here 
param_dist = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', 0.5],
    'bootstrap': [True, False]
}

# using GridsearchCV/ RandomsearchCV (MVP)
rf_model = RandomForestRegressor(random_state=42)
random_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_dist,
    n_iter=50,  # number of random combinations to try
    cv=5,  # 5-fold cross-validation
    verbose=3,  # increase verbosity to 3 to show iteration details
    random_state=42,
    n_jobs=-1,  # use all CPU cores
    scoring='neg_mean_absolute_error'  # we want to minimize MAE
)


X_train_imputed = pd.read_csv('X_train_imputed.csv').values
y_train_imputed = pd.read_csv('y_train_imputed.csv').values

# Ensure y is 1D (using ravel or flatten)
y_train_imputed = y_train_imputed.ravel()  # or y_train_imputed.flatten()

# Then fit the model
random_search.fit(X_train_imputed, y_train_imputed)

# Best hyperparameters found by RandomizedSearchCV
# store it
print(f"Best Hyperparameters: {random_search.best_params_}")

# or your custom functions








best_rf_model = random_search.best_estimator_
X_test_imputed = pd.read_csv('X_test_imputed.csv').values
y_test_imputed = pd.read_csv('y_test_imputed.csv').values


y_test_pred = best_rf_model.predict(X_test_imputed)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test_imputed, y_test_pred)
rmse = np.sqrt(mean_squared_error(y_test_imputed, y_test_pred))
r2 = r2_score(y_test_imputed, y_test_pred)
n = X_test_imputed.shape[0]
p = X_test_imputed.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)

# Display the performance of the best model
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R^2: {r2:.4f}")
print(f"Adjusted R^2: {adjusted_r2:.4f}")

# Feature importance from the best model
importances = best_rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# Display the top 20 feature importances
top_n = 20
for f in range(top_n):
    print(f"{X_train_imputed.columns[indices[f]]}: {importances[indices[f]]:.4f}")

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.title("Top 20 Feature Importances")
plt.barh(range(top_n), importances[indices[:top_n]], align="center")
plt.yticks(range(top_n), X_train_imputed.columns[indices[:top_n]])  # Changed this line to use the correct columns
plt.xlabel("Importance")
plt.show()









import os
import joblib  # or you can use pickle if you prefer
from sklearn.ensemble import RandomForestRegressor

# Ensure the models directory exists
models_dir = 'models'
if not os.path.exists(models_dir):
    os.makedirs(models_dir)

# Save the best model using joblib
best_rf_model = random_search.best_estimator_

# Define the path where you want to save the model
model_filename = os.path.join(models_dir, 'best_rf_model.pkl')

# Save the model
joblib.dump(best_rf_model, model_filename)

print(f"Model saved to {model_filename}")






# Build pipeline here





# save your pipeline here
